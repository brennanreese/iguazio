{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Operator + MLRun + Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-04-26 10:11:35,590 [info] created and saved project spark-delta\n"
     ]
    }
   ],
   "source": [
    "from mlrun import get_or_create_project, code_to_function\n",
    "\n",
    "project_name=\"spark-delta\"\n",
    "project = get_or_create_project(name=project_name, context='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import new_function\n",
    "fx = new_function(name='read-delta',\n",
    "                  command='v3io://projects/spark-delta/spark_delta_lake.py',\n",
    "                  kind='spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set spark driver config (gpu also supported)\n",
    "fx.with_driver_limits(cpu=\"1024m\")\n",
    "fx.with_driver_requests(cpu=1, mem=\"512m\")\n",
    "\n",
    "# set spark executor config \n",
    "fx.with_executor_limits(cpu=\"1024m\")\n",
    "fx.with_executor_requests(cpu=1, mem=\"512m\")\n",
    "\n",
    "# adds fuse, daemon, & iguazio's jar support\n",
    "fx.with_igz_spark()\n",
    "\n",
    "# pass some args\n",
    "fx.spec.args = []\n",
    "\n",
    "# add python modules for delta lake support\n",
    "#fx.spec.build.commands = ['pip install -U delta-spark==1.0.0']\n",
    "fx.spec.spark_conf['spark.jars.packages'] = 'io.delta:delta-core_2.12:1.0.0'\n",
    "fx.spec.spark_conf['spark.sql.extensions'] = 'io.delta.sql.DeltaSparkSessionExtension'\n",
    "fx.spec.spark_conf['spark.sql.catalog.spark_catalog'] = 'org.apache.spark.sql.delta.catalog.DeltaCatalog'\n",
    "\n",
    "# number of executors\n",
    "fx.spec.replicas=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-04-26 13:20:42,116 [info] running build to add mlrun package, set with_mlrun=False to skip if its already in the image\n",
      "> 2022-04-26 13:20:42,495 [info] Started building image: .mlrun/func-spark-delta-read-delta:latest\n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest gcr.io/iguazio/spark-app:3.2.2-b128.20220214155542 \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image gcr.io/iguazio/spark-app:3.2.2-b128.20220214155542 from registry gcr.io \n",
      "\u001b[36mINFO\u001b[0m[0000] Built cross stage deps: map[]                \n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest gcr.io/iguazio/spark-app:3.2.2-b128.20220214155542 \n",
      "\u001b[36mINFO\u001b[0m[0000] Returning cached image manifest              \n",
      "\u001b[36mINFO\u001b[0m[0000] Executing 0 build triggers                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Unpacking rootfs as cmd RUN python -m pip install \"mlrun[complete]==0.10.0\" requires it. \n",
      "\u001b[36mINFO\u001b[0m[0045] RUN python -m pip install \"mlrun[complete]==0.10.0\" \n",
      "\u001b[36mINFO\u001b[0m[0045] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0061] cmd: /bin/sh                                 \n",
      "\u001b[36mINFO\u001b[0m[0061] args: [-c python -m pip install \"mlrun[complete]==0.10.0\"] \n",
      "\u001b[36mINFO\u001b[0m[0061] util.Lookup returned: &{Uid:1000 Gid:1000 Username:iguazio Name: HomeDir:/igz} \n",
      "\u001b[36mINFO\u001b[0m[0061] performing slow lookup of group ids for iguazio \n",
      "\u001b[36mINFO\u001b[0m[0061] Running: [/bin/sh -c python -m pip install \"mlrun[complete]==0.10.0\"] \n",
      "Collecting mlrun[complete]==0.10.0\n",
      "  Downloading mlrun-0.10.0-py3-none-any.whl (803 kB)\n",
      "Collecting pydantic~=1.5\n",
      "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "Collecting pandas~=1.2\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Requirement already satisfied: chardet<4.0,>=3.0.2 in /conda/lib/python3.7/site-packages (from mlrun[complete]==0.10.0) (3.0.4)\n",
      "Collecting v3iofs~=0.1.7\n",
      "  Downloading v3iofs-0.1.10-py3-none-any.whl (13 kB)\n",
      "Collecting google-auth<2.0dev,>=1.25.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting storey~=0.10.5\n",
      "  Downloading storey-0.10.5-py3-none-any.whl (117 kB)\n",
      "Collecting python-dotenv~=0.17.0\n",
      "  Downloading python_dotenv-0.17.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tabulate~=0.8.6\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting sqlalchemy~=1.3\n",
      "  Downloading SQLAlchemy-1.4.35-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting orjson<3.4,>=3\n",
      "  Downloading orjson-3.3.1-cp37-cp37m-manylinux2014_x86_64.whl (208 kB)\n",
      "Collecting requests~=2.22\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting kubernetes~=12.0\n",
      "  Downloading kubernetes-12.0.1-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting alembic<1.6.0,~=1.4\n",
      "  Downloading alembic-1.5.8-py2.py3-none-any.whl (159 kB)\n",
      "Collecting nuclio-jupyter~=0.8.22\n",
      "  Downloading nuclio_jupyter-0.8.23-py3-none-any.whl (49 kB)\n",
      "Collecting cryptography<3.4,~=3.0\n",
      "  Downloading cryptography-3.3.2-cp36-abi3-manylinux2010_x86_64.whl (2.6 MB)\n",
      "Collecting pyarrow<6,>=1\n",
      "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "Collecting click~=7.0\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting distributed~=2021.11.2\n",
      "  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n",
      "Collecting fastapi~=0.67.0\n",
      "  Downloading fastapi-0.67.0-py3-none-any.whl (51 kB)\n",
      "Collecting aiohttp~=3.8\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Collecting dask~=2021.11.2\n",
      "  Downloading dask-2021.11.2-py3-none-any.whl (1.0 MB)\n",
      "Collecting deepdiff~=5.0\n",
      "  Downloading deepdiff-5.8.0-py3-none-any.whl (69 kB)\n",
      "Collecting nest-asyncio~=1.0\n",
      "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
      "Collecting inflection~=0.5.0\n",
      "  Downloading inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting semver~=2.13\n",
      "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting pymysql~=1.0\n",
      "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "Collecting fsspec~=2021.8.1\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "Collecting GitPython~=3.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Collecting pyyaml~=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "Collecting v3io~=0.5.13\n",
      "  Downloading v3io-0.5.15-py3-none-any.whl (49 kB)\n",
      "Collecting v3io-frames~=0.10.2\n",
      "  Downloading v3io_frames-0.10.2-py3-none-any.whl (35 kB)\n",
      "Collecting mergedeep~=1.3\n",
      "  Downloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
      "Collecting humanfriendly~=8.2\n",
      "  Downloading humanfriendly-8.2-py2.py3-none-any.whl (86 kB)\n",
      "Collecting ipython~=7.0\n",
      "  Downloading ipython-7.32.0-py3-none-any.whl (793 kB)\n",
      "Collecting kfp~=1.8.0\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "Collecting numpy<1.22.0,>=1.16.5\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting ipykernel~=5.0\n",
      "  Downloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\n",
      "Collecting azure-keyvault-secrets~=4.2; extra == \"complete\"\n",
      "  Downloading azure_keyvault_secrets-4.4.0-py3-none-any.whl (291 kB)\n",
      "Collecting plotly~=5.4; extra == \"complete\"\n",
      "  Downloading plotly-5.7.0-py2.py3-none-any.whl (28.8 MB)\n",
      "Collecting azure-storage-blob~=12.0; extra == \"complete\"\n",
      "  Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB)\n",
      "Collecting adlfs~=2021.8.1; extra == \"complete\"\n",
      "  Downloading adlfs-2021.8.2.tar.gz (38 kB)\n",
      "Collecting aiobotocore~=1.4.0; extra == \"complete\"\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "Collecting boto3<1.17.107,~=1.9; extra == \"complete\"\n",
      "  Downloading boto3-1.17.106-py2.py3-none-any.whl (131 kB)\n",
      "Collecting botocore<1.20.107,>=1.20.106; extra == \"complete\"\n",
      "  Downloading botocore-1.20.106-py2.py3-none-any.whl (7.7 MB)\n",
      "Collecting azure-identity~=1.5; extra == \"complete\"\n",
      "  Downloading azure_identity-1.9.0-py3-none-any.whl (134 kB)\n",
      "Collecting gcsfs~=2021.8.1; extra == \"complete\"\n",
      "  Downloading gcsfs-2021.8.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting s3fs~=2021.8.1; extra == \"complete\"\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->mlrun[complete]==0.10.0) (1.12.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->mlrun[complete]==0.10.0) (41.0.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting grpcio-tools<1.42,>1.34.0\n",
      "  Downloading grpcio_tools-1.41.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "Collecting grpcio<1.42,>1.34.0\n",
      "  Downloading grpcio-1.41.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Collecting greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\"))))))\n",
      "  Downloading greenlet-1.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (150 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /conda/lib/python3.7/site-packages (from requests~=2.22->mlrun[complete]==0.10.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /conda/lib/python3.7/site-packages (from requests~=2.22->mlrun[complete]==0.10.0) (2021.10.8)\n",
      "Collecting charset-normalizer~=2.0.0; python_version >= \"3\"\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
      "Collecting jinja2~=3.0.0\n",
      "  Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
      "Collecting notebook>=5.2.0\n",
      "  Downloading notebook-6.4.11-py3-none-any.whl (9.9 MB)\n",
      "Collecting nbconvert>=5.4\n",
      "  Downloading nbconvert-6.5.0-py3-none-any.whl (561 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /conda/lib/python3.7/site-packages (from cryptography<3.4,~=3.0->mlrun[complete]==0.10.0) (1.12.2)\n",
      "Collecting cloudpickle>=1.5.0\n",
      "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.1.0-py3-none-any.whl (11 kB)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting psutil>=5.0\n",
      "  Downloading psutil-5.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
      "Collecting toolz>=0.8.2\n",
      "  Downloading toolz-0.11.2-py3-none-any.whl (55 kB)\n",
      "Collecting tornado>=5; python_version < \"3.8\"\n",
      "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
      "Collecting starlette==0.14.2\n",
      "  Downloading starlette-0.14.2-py3-none-any.whl (60 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "Collecting asynctest==0.13.0; python_version < \"3.8\"\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
      "Collecting ordered-set<4.2.0,>=4.1.0\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting future>=0.18.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting ujson>=3.0.0\n",
      "  Downloading ujson-5.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
      "Collecting googleapis-common-protos>=1.5.3\n",
      "  Downloading googleapis_common_protos-1.56.0-py2.py3-none-any.whl (241 kB)\n",
      "Collecting jedi>=0.16\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting matplotlib-inline\n",
      "  Downloading matplotlib_inline-0.1.3-py3-none-any.whl (8.2 kB)\n",
      "Collecting pexpect>4.3; sys_platform != \"win32\"\n",
      "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
      "Collecting pickleshare\n",
      "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Collecting decorator\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting traitlets>=4.2\n",
      "  Downloading traitlets-5.1.1-py3-none-any.whl (102 kB)\n",
      "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
      "  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n",
      "Collecting pygments\n",
      "  Downloading Pygments-2.12.0-py3-none-any.whl (1.1 MB)\n",
      "Collecting backcall\n",
      "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting absl-py<2,>=0.9\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.7.2-py3-none-any.whl (114 kB)\n",
      "Collecting google-cloud-storage<2,>=1.20.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.1.tar.gz (54 kB)\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.14-py3-none-any.whl (32 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
      "  Downloading kfp_pipeline_spec-0.1.14-py3-none-any.whl (18 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Collecting protobuf<4,>=3.13.0\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting jupyter-client\n",
      "  Downloading jupyter_client-7.3.0-py3-none-any.whl (130 kB)\n",
      "Collecting ipython-genutils\n",
      "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting msrest>=0.6.21\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "Collecting azure-core<2.0.0,>=1.20.0\n",
      "  Downloading azure_core-1.23.1-py3-none-any.whl (178 kB)\n",
      "Collecting azure-common~=1.1\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting azure-datalake-store<0.1,>=0.0.46\n",
      "  Downloading azure_datalake_store-0.0.52-py2.py3-none-any.whl (61 kB)\n",
      "Collecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.14.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.10.0-py3-none-any.whl (23 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.5.0,>=0.4.0\n",
      "  Downloading s3transfer-0.4.2-py2.py3-none-any.whl (79 kB)\n",
      "Collecting msal<2.0.0,>=1.12.0\n",
      "  Downloading msal-1.17.0-py2.py3-none-any.whl (79 kB)\n",
      "Collecting msal-extensions~=0.3.0\n",
      "  Downloading msal_extensions-0.3.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-0.5.1-py2.py3-none-any.whl (19 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting MarkupSafe>=0.9.2\n",
      "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting nbformat\n",
      "  Downloading nbformat-5.3.0-py3-none-any.whl (73 kB)\n",
      "Collecting jupyter-core>=4.6.1\n",
      "  Downloading jupyter_core-4.10.0-py3-none-any.whl (87 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
      "Collecting terminado>=0.8.3\n",
      "  Downloading terminado-0.13.3-py3-none-any.whl (14 kB)\n",
      "Collecting pyzmq>=17\n",
      "  Downloading pyzmq-22.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
      "Collecting Send2Trash>=1.8.0\n",
      "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting tinycss2\n",
      "  Downloading tinycss2-1.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting entrypoints>=0.2.2\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.6.0-py3-none-any.whl (70 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting bleach\n",
      "  Downloading bleach-5.0.0-py3-none-any.whl (160 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Requirement already satisfied: pycparser in /conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography<3.4,~=3.0->mlrun[complete]==0.10.0) (2.19)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Downloading pyparsing-3.0.8-py3-none-any.whl (98 kB)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting parso<0.9.0,>=0.8.0\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "Collecting ptyprocess>=0.5\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting wcwidth\n",
      "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=1.3.0; python_version >= \"3.6\"\n",
      "  Downloading google_resumable_media-2.3.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=1.6.0; python_version >= \"3.6\"\n",
      "  Downloading google_cloud_core-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "Collecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.18.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "Requirement already satisfied: wheel in /conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp~=1.8.0->mlrun[complete]==0.10.0) (0.33.1)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting adal>=0.4.2\n",
      "  Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
      "Collecting PyJWT[crypto]<3,>=1.0.0\n",
      "  Downloading PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting portalocker<3,>=1.0; python_version >= \"3.5\" and platform_system != \"Windows\"\n",
      "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting fastjsonschema\n",
      "  Downloading fastjsonschema-2.15.3-py3-none-any.whl (22 kB)\n",
      "Collecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
      "Collecting webencodings>=0.4\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38 kB)\n",
      "Building wheels for collected packages: kfp, adlfs, aiobotocore, future, kfp-server-api, strip-hints, fire, termcolor\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419052 sha256=b406e02b06235b3b492693310ad59e4d966bce5276d7e1757594ff2401f07c4a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hm5mnob/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "  Building wheel for adlfs (setup.py): started\n",
      "  Building wheel for adlfs (setup.py): finished with status 'done'\n",
      "  Created wheel for adlfs: filename=adlfs-2021.8.2-py3-none-any.whl size=21470 sha256=83a1243d0f1caa009d57a17ee82e47ffec7c0f11d2af310f1e46d938dc70e1cb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hm5mnob/wheels/0d/88/1d/e06072abb7fb4d59b5cf94e194e53017dfa2dc47af4dec88b7\n",
      "  Building wheel for aiobotocore (setup.py): started\n",
      "  Building wheel for aiobotocore (setup.py): finished with status 'done'\n",
      "  Created wheel for aiobotocore: filename=aiobotocore-1.4.2-py3-none-any.whl size=49909 sha256=fed719e4dd9a3c803117f6e232f4eee6800db4967abcd3035066920d1c6e5727\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hm5mnob/wheels/33/e7/d9/b297a9aa9c43d56bc2463e6e2771655ff638f30b30f0b61fcb\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491056 sha256=459f9c65f38db84a5ef70f3c5fa698117d6990f1f8cdd436e231df82bc436c85\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hm5mnob/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.1-py3-none-any.whl size=95547 sha256=7162961f701cd77f45b05c126feae5cf796d45f6ec5470accffc821ba155b78e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hm5mnob/wheels/f5/4e/2e/6795bd3ed456a43652e7de100aca275ec179c9a8dfbcc65626\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=0b141e55d34a3cbd03122975dc11bae78f130ddd59901f0423223711c50499f5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hm5mnob/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115929 sha256=a23fb0a3c5d87aa1f4e96cabedb18bbc16ba36db13ac36f64d6a957fedd314a2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hm5mnob/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=0d1e154fb002270624a83fb5afb7fe8062c345196d4298fc17154f9ae282c459\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3hm5mnob/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built kfp adlfs aiobotocore future kfp-server-api strip-hints fire termcolor\n",
      "Installing collected packages: typing-extensions, pydantic, python-dateutil, pytz, numpy, pandas, future, ujson, urllib3, charset-normalizer, requests, v3io, fsspec, v3iofs, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, protobuf, grpcio, grpcio-tools, googleapis-common-protos, v3io-frames, pyarrow, multidict, yarl, async-timeout, frozenlist, aiosignal, asynctest, attrs, aiohttp, storey, python-dotenv, tabulate, zipp, importlib-metadata, greenlet, sqlalchemy, orjson, websocket-client, oauthlib, requests-oauthlib, pyyaml, kubernetes, python-editor, MarkupSafe, Mako, alembic, jinja2, parso, jedi, traitlets, matplotlib-inline, ptyprocess, pexpect, pickleshare, decorator, wcwidth, prompt-toolkit, pygments, backcall, ipython, jupyter-core, fastjsonschema, pyrsistent, jsonschema, nbformat, nest-asyncio, jupyterlab-pygments, webencodings, tinycss2, entrypoints, mistune, pyzmq, tornado, jupyter-client, nbclient, pandocfilters, defusedxml, bleach, pyparsing, packaging, soupsieve, beautifulsoup4, nbconvert, prometheus-client, terminado, argon2-cffi-bindings, argon2-cffi, ipython-genutils, Send2Trash, ipykernel, notebook, jmespath, botocore, s3transfer, boto3, nuclio-jupyter, cryptography, click, cloudpickle, tblib, heapdict, zict, msgpack, sortedcontainers, psutil, toolz, locket, partd, dask, distributed, starlette, fastapi, ordered-set, deepdiff, inflection, semver, pymysql, smmap, gitdb, GitPython, mergedeep, humanfriendly, absl-py, google-api-core, google-crc32c, google-resumable-media, google-cloud-core, google-cloud-storage, uritemplate, httplib2, google-auth-httplib2, google-api-python-client, requests-toolbelt, kfp-server-api, wrapt, Deprecated, strip-hints, docstring-parser, kfp-pipeline-spec, termcolor, fire, typer, kfp, isodate, msrest, azure-core, azure-common, azure-keyvault-secrets, tenacity, plotly, azure-storage-blob, PyJWT, adal, azure-datalake-store, msal, portalocker, msal-extensions, azure-identity, adlfs, aioitertools, aiobotocore, google-auth-oauthlib, gcsfs, s3fs, mlrun\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.1\n",
      "    Uninstalling urllib3-1.24.1:\n",
      "      Successfully uninstalled urllib3-1.24.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.21.0\n",
      "    Uninstalling requests-2.21.0:\n",
      "      Successfully uninstalled requests-2.21.0\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 2.6.1\n",
      "    Uninstalling cryptography-2.6.1:\n",
      "      Successfully uninstalled cryptography-2.6.1\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\n",
      "kfp 1.8.12 requires typing-extensions<4,>=3.7.4; python_version < \"3.9\", but you'll have typing-extensions 4.2.0 which is incompatible.\n",
      "Successfully installed Deprecated-1.2.13 GitPython-3.1.27 Mako-1.2.0 MarkupSafe-2.1.1 PyJWT-2.3.0 Send2Trash-1.8.0 absl-py-1.0.0 adal-1.2.7 adlfs-2021.8.2 aiobotocore-1.4.2 aiohttp-3.8.1 aioitertools-0.10.0 aiosignal-1.2.0 alembic-1.5.8 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-timeout-4.0.2 asynctest-0.13.0 attrs-21.4.0 azure-common-1.1.28 azure-core-1.23.1 azure-datalake-store-0.0.52 azure-identity-1.9.0 azure-keyvault-secrets-4.4.0 azure-storage-blob-12.11.0 backcall-0.2.0 beautifulsoup4-4.11.1 bleach-5.0.0 boto3-1.17.106 botocore-1.20.106 cachetools-4.2.4 charset-normalizer-2.0.12 click-7.1.2 cloudpickle-2.0.0 cryptography-3.3.2 dask-2021.11.2 decorator-5.1.1 deepdiff-5.8.0 defusedxml-0.7.1 distributed-2021.11.2 docstring-parser-0.14 entrypoints-0.4 fastapi-0.67.0 fastjsonschema-2.15.3 fire-0.4.0 frozenlist-1.3.0 fsspec-2021.8.1 future-0.18.2 gcsfs-2021.8.1 gitdb-4.0.9 google-api-core-2.7.2 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.5.1 google-cloud-core-2.3.0 google-cloud-storage-1.44.0 google-crc32c-1.3.0 google-resumable-media-2.3.2 googleapis-common-protos-1.56.0 greenlet-1.1.2 grpcio-1.41.1 grpcio-tools-1.41.1 heapdict-1.0.1 httplib2-0.20.4 humanfriendly-8.2 importlib-metadata-4.11.3 inflection-0.5.1 ipykernel-5.5.6 ipython-7.32.0 ipython-genutils-0.2.0 isodate-0.6.1 jedi-0.18.1 jinja2-3.0.3 jmespath-0.10.0 jsonschema-3.2.0 jupyter-client-7.3.0 jupyter-core-4.10.0 jupyterlab-pygments-0.2.2 kfp-1.8.12 kfp-pipeline-spec-0.1.14 kfp-server-api-1.8.1 kubernetes-12.0.1 locket-1.0.0 matplotlib-inline-0.1.3 mergedeep-1.3.4 mistune-0.8.4 mlrun-0.10.0 msal-1.17.0 msal-extensions-0.3.1 msgpack-1.0.3 msrest-0.6.21 multidict-6.0.2 nbclient-0.6.0 nbconvert-6.5.0 nbformat-5.3.0 nest-asyncio-1.5.5 notebook-6.4.11 nuclio-jupyter-0.8.23 numpy-1.21.6 oauthlib-3.2.0 ordered-set-4.1.0 orjson-3.3.1 packaging-21.3 pandas-1.3.5 pandocfilters-1.5.0 parso-0.8.3 partd-1.2.0 pexpect-4.8.0 pickleshare-0.7.5 plotly-5.7.0 portalocker-2.4.0 prometheus-client-0.14.1 prompt-toolkit-3.0.29 protobuf-3.20.1 psutil-5.9.0 ptyprocess-0.7.0 pyarrow-5.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.9.0 pygments-2.12.0 pymysql-1.0.2 pyparsing-3.0.8 pyrsistent-0.18.1 python-dateutil-2.8.2 python-dotenv-0.17.1 python-editor-1.0.4 pytz-2022.1 pyyaml-5.4.1 pyzmq-22.3.0 requests-2.27.1 requests-oauthlib-1.3.1 requests-toolbelt-0.9.1 rsa-4.8 s3fs-2021.8.1 s3transfer-0.4.2 semver-2.13.0 smmap-5.0.0 sortedcontainers-2.4.0 soupsieve-2.3.2.post1 sqlalchemy-1.4.35 starlette-0.14.2 storey-0.10.5 strip-hints-0.1.10 tabulate-0.8.9 tblib-1.7.0 tenacity-8.0.1 termcolor-1.1.0 terminado-0.13.3 tinycss2-1.1.1 toolz-0.11.2 tornado-6.1 traitlets-5.1.1 typer-0.4.1 typing-extensions-4.2.0 ujson-5.2.0 uritemplate-3.0.1 urllib3-1.26.9 v3io-0.5.15 v3io-frames-0.10.2 v3iofs-0.1.10 wcwidth-0.2.5 webencodings-0.5.1 websocket-client-1.3.2 wrapt-1.14.0 yarl-1.7.2 zict-2.1.0 zipp-3.8.0\n",
      "\u001b[36mINFO\u001b[0m[0135] Taking snapshot of full filesystem...        \n",
      "\u001b[36mINFO\u001b[0m[0157] Pushing image to docker-registry.default-tenant.app.us-sales-322.iguazio-cd1.com:80/mlrun/func-spark-delta-read-delta:latest \n",
      "\u001b[36mINFO\u001b[0m[0159] Pushed image to 1 destinations               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-04-26 13:31:13,716 [info] starting run read-delta uid=7714d2fc2350485188dcff2f0d3af951 DB=https://mlrun-api.default-tenant.app.us-sales-322.iguazio-cd1.com\n",
      "++ id -u\n",
      "+ myuid=1000\n",
      "++ id -g\n",
      "+ mygid=1000\n",
      "+ set +e\n",
      "++ getent passwd 1000\n",
      "+ uidentry=iguazio:x:1000:1000::/igz:/bin/bash\n",
      "+ set -e\n",
      "+ '[' -z iguazio:x:1000:1000::/igz:/bin/bash ']'\n",
      "+ SPARK_CLASSPATH=':/spark/jars/*'\n",
      "+ env\n",
      "+ grep SPARK_JAVA_OPT_\n",
      "+ sort -t_ -k4 -n\n",
      "+ sed 's/[^=]*=\\(.*\\)/\\1/g'\n",
      "+ readarray -t SPARK_EXECUTOR_JAVA_OPTS\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -z ']'\n",
      "+ '[' -n /hadoop ']'\n",
      "+ '[' -z '' ']'\n",
      "++ /hadoop/bin/hadoop classpath\n",
      "+ export 'SPARK_DIST_CLASSPATH=/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/lib/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar'\n",
      "+ SPARK_DIST_CLASSPATH='/hadoop/etc/hadoop:/hadoop/share/hadoop/common/lib/*:/hadoop/share/hadoop/common/*:/hadoop/share/hadoop/hdfs:/hadoop/share/hadoop/hdfs/lib/*:/hadoop/share/hadoop/hdfs/*:/hadoop/share/hadoop/mapreduce/lib/*:/hadoop/share/hadoop/mapreduce/*:/hadoop/share/hadoop/yarn:/hadoop/share/hadoop/yarn/lib/*:/hadoop/share/hadoop/yarn/*:/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar:/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ '[' -z x ']'\n",
      "+ SPARK_CLASSPATH='/opt/spark/conf:/hadoop/etc/hadoop::/spark/jars/*'\n",
      "+ case \"$1\" in\n",
      "+ shift 1\n",
      "+ CMD=(\"$SPARK_HOME/bin/spark-submit\" --conf \"spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS\" --deploy-mode client \"$@\")\n",
      "+ exec /usr/bin/tini -s -- /spark/bin/spark-submit --conf spark.driver.bindAddress=172.31.1.146 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.deploy.PythonRunner v3io://projects/spark-delta/spark_delta_lake.py\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      ":: loading settings :: url = jar:file:/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /igz/.ivy2/cache\n",
      "The jars for the packages stored in: /igz/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c3c8407f-e44e-4de9-a000-4d58a03ce143;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.0/delta-core_2.12-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.0!delta-core_2.12.jar (107ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (71ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (22ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (39ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (19ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (17ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (15ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (363ms)\n",
      ":: resolution report :: resolve 4056ms :: artifacts dl 658ms\n",
      "\t:: modules in use:\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   8   |   8   |   0   ||   8   |   8   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c3c8407f-e44e-4de9-a000-4d58a03ce143\n",
      "\tconfs: [default]\n",
      "\t8 artifacts copied, 0 already retrieved (15452kB/19ms)\n",
      "2022-04-26 17:31:31,344 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "> 2022-04-26 17:31:43,036 [info] logging run results to: http://mlrun-api:8080\n",
      "2022-04-26 17:31:43,212 INFO spark.SparkContext: Running Spark version 3.1.2\n",
      "2022-04-26 17:31:43,241 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-04-26 17:31:43,241 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2022-04-26 17:31:43,241 INFO resource.ResourceUtils: ==============================================================\n",
      "2022-04-26 17:31:43,241 INFO spark.SparkContext: Submitted application: sparky\n",
      "2022-04-26 17:31:43,309 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2022-04-26 17:31:43,315 INFO resource.ResourceProfile: Limiting resource is cpus at 1 tasks per executor\n",
      "2022-04-26 17:31:43,316 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2022-04-26 17:31:43,361 INFO spark.SecurityManager: Changing view acls to: iguazio\n",
      "2022-04-26 17:31:43,361 INFO spark.SecurityManager: Changing modify acls to: iguazio\n",
      "2022-04-26 17:31:43,361 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2022-04-26 17:31:43,361 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2022-04-26 17:31:43,361 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(iguazio); groups with view permissions: Set(); users  with modify permissions: Set(iguazio); groups with modify permissions: Set()\n",
      "2022-04-26 17:31:43,736 INFO util.Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "2022-04-26 17:31:43,812 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2022-04-26 17:31:43,881 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2022-04-26 17:31:43,930 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2022-04-26 17:31:43,930 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2022-04-26 17:31:43,934 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2022-04-26 17:31:43,946 INFO storage.DiskBlockManager: Created local directory at /var/data/spark-e505979c-3004-422b-90d4-dcde5f7b8903/blockmgr-2fb31c86-32df-4f2b-b74a-bb7f952962b6\n",
      "2022-04-26 17:31:43,966 INFO memory.MemoryStore: MemoryStore started with capacity 117.0 MiB\n",
      "2022-04-26 17:31:44,013 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2022-04-26 17:31:44,133 INFO util.log: Logging initialized @21160ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2022-04-26 17:31:44,230 INFO server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 11.0.11+9-LTS\n",
      "2022-04-26 17:31:44,246 INFO server.Server: Started @21274ms\n",
      "2022-04-26 17:31:44,327 INFO server.AbstractConnector: Started ServerConnector@27bab6a4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2022-04-26 17:31:44,327 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2022-04-26 17:31:44,352 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f2d7efd{/jobs,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,410 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@235a574d{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,411 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@416a13{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,412 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@ffb1e2a{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,412 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70f307ed{/stages,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,413 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c2e0c7b{/stages/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,413 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4902a127{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,414 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@694a99c0{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,414 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@90dfd8c{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,415 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30b263b5{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,415 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2dfdb7d2{/storage,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,416 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58d6bfb2{/storage/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,416 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52785e7{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,417 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45b3871b{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,417 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e098eff{/environment,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,418 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@431ecded{/environment/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,419 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7796b45d{/executors,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,419 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38b36ea3{/executors/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,420 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@accc170{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,421 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19b4d960{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,429 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68bbe593{/static,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,429 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36ec3e43{/,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,430 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70e95fdc{/api,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,431 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57933363{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,431 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28111da6{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:44,432 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc:4040\n",
      "2022-04-26 17:31:44,513 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-hcfs_2.12.jar at file:/spark/v3io-libs/v3io-hcfs_2.12.jar with timestamp 1650994303152\n",
      "2022-04-26 17:31:44,513 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-streaming_2.12.jar at file:/spark/v3io-libs/v3io-spark3-streaming_2.12.jar with timestamp 1650994303152\n",
      "2022-04-26 17:31:44,513 INFO spark.SparkContext: Added JAR local:///spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar at file:/spark/v3io-libs/v3io-spark3-object-dataframe_2.12.jar with timestamp 1650994303152\n",
      "2022-04-26 17:31:44,513 INFO spark.SparkContext: Added JAR local:///igz/java/libs/scala-library-2.12.14.jar at file:/igz/java/libs/scala-library-2.12.14.jar with timestamp 1650994303152\n",
      "2022-04-26 17:31:44,513 INFO spark.SparkContext: Added JAR local:///spark/jars/jmx_prometheus_javaagent-0.16.1.jar at file:/spark/jars/jmx_prometheus_javaagent-0.16.1.jar with timestamp 1650994303152\n",
      "2022-04-26 17:31:44,514 WARN spark.SparkContext: File with 'local' scheme local:///igz/java/libs/v3io-pyspark.zip is not supported to add to file server, since it is already available on every node.\n",
      "2022-04-26 17:31:44,645 INFO k8s.SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file\n",
      "2022-04-26 17:31:45,912 INFO k8s.ExecutorPodsAllocator: Going to request 1 executors from Kubernetes for ResourceProfile Id: 0, target: 1 running: 0.\n",
      "2022-04-26 17:31:46,117 INFO features.BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script\n",
      "2022-04-26 17:31:46,141 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.\n",
      "2022-04-26 17:31:46,141 INFO netty.NettyBlockTransferService: Server created on read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc:7079\n",
      "2022-04-26 17:31:46,143 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2022-04-26 17:31:46,214 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-04-26 17:31:46,217 INFO storage.BlockManagerMasterEndpoint: Registering block manager read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc:7079 with 117.0 MiB RAM, BlockManagerId(driver, read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-04-26 17:31:46,220 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-04-26 17:31:46,221 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc, 7079, None)\n",
      "2022-04-26 17:31:46,409 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ef05ed{/metrics/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:56,269 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.0.108:44318) with ID 1,  ResourceProfileId 0\n",
      "2022-04-26 17:31:56,320 INFO k8s.KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "2022-04-26 17:31:56,361 INFO storage.BlockManagerMasterEndpoint: Registering block manager 172.31.0.108:39681 with 117.0 MiB RAM, BlockManagerId(1, 172.31.0.108, 39681, None)\n",
      "2022-04-26 17:31:56,560 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/spark/spark-warehouse').\n",
      "2022-04-26 17:31:56,560 INFO internal.SharedState: Warehouse path is 'file:/spark/spark-warehouse'.\n",
      "2022-04-26 17:31:56,622 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@af3de26{/SQL,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:56,623 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4123a9f2{/SQL/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:56,624 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ed1452f{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:56,625 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1584e50b{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:31:56,626 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4db836ef{/static/sql,null,AVAILABLE,@Spark}\n",
      "2022-04-26 17:32:01,151 INFO storage.DelegatingLogStore: LogStore org.apache.spark.sql.delta.storage.HDFSLogStore is used for scheme v3io\n",
      "2022-04-26 17:32:01,363 INFO delta.DeltaLog: Creating initial snapshot without metadata, because the directory is empty\n",
      "2022-04-26 17:32:01,388 INFO delta.InitialSnapshot: [tableId=ff15c2b5-ce6a-4238-bfc0-10d7c43627b2] Created snapshot InitialSnapshot(path=v3io://projects/spark-delta/deltab/_delta_log, version=-1, metadata=Metadata(7cc0c920-7d31-49bb-b14c-94410ee8b843,null,null,Format(parquet,Map()),null,List(),Map(),Some(1650994321385)), logSegment=LogSegment(v3io://projects/spark-delta/deltab/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)\n",
      "2022-04-26 17:32:01,553 INFO delta.DeltaLog: No delta log found for the Delta table at v3io://projects/spark-delta/deltab/_delta_log\n",
      "2022-04-26 17:32:01,553 INFO delta.InitialSnapshot: [tableId=7cc0c920-7d31-49bb-b14c-94410ee8b843] Created snapshot InitialSnapshot(path=v3io://projects/spark-delta/deltab/_delta_log, version=-1, metadata=Metadata(9d8f7791-9bb9-4f50-825c-13f0163fa573,null,null,Format(parquet,Map()),null,List(),Map(),Some(1650994321553)), logSegment=LogSegment(v3io://projects/spark-delta/deltab/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)\n",
      "2022-04-26 17:32:01,659 INFO delta.OptimisticTransaction: [tableId=9d8f7791-9bb9-4f50-825c-13f0163fa573] Updated metadata from - to Metadata(853075a9-2c48-43b6-9a6e-98b84c52273e,null,null,Format(parquet,Map()),{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},List(),Map(),Some(1650994321627))\n",
      "2022-04-26 17:32:01,950 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "2022-04-26 17:32:02,514 INFO codegen.CodeGenerator: Code generated in 386.822414 ms\n",
      "2022-04-26 17:32:02,737 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "2022-04-26 17:32:02,749 INFO scheduler.DAGScheduler: Got job 0 (save at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "2022-04-26 17:32:02,750 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (save at NativeMethodAccessorImpl.java:0)\n",
      "2022-04-26 17:32:02,750 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-04-26 17:32:02,751 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-04-26 17:32:02,811 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2022-04-26 17:32:03,021 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 257.4 KiB, free 116.7 MiB)\n",
      "2022-04-26 17:32:03,048 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 94.8 KiB, free 116.6 MiB)\n",
      "2022-04-26 17:32:03,109 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc:7079 (size: 94.8 KiB, free: 116.9 MiB)\n",
      "2022-04-26 17:32:03,111 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1388\n",
      "2022-04-26 17:32:03,122 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2022-04-26 17:32:03,123 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "2022-04-26 17:32:03,216 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.31.0.108, executor 1, partition 0, PROCESS_LOCAL, 4597 bytes) taskResourceAssignments Map()\n",
      "2022-04-26 17:32:03,521 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.31.0.108:39681 (size: 94.8 KiB, free: 116.9 MiB)\n",
      "2022-04-26 17:32:04,152 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.31.0.108, executor 1, partition 1, PROCESS_LOCAL, 4597 bytes) taskResourceAssignments Map()\n",
      "2022-04-26 17:32:04,164 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.31.0.108 executor 1): java.lang.ClassNotFoundException: org.apache.spark.sql.delta.files.DelayedCommitProtocol\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1995)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "2022-04-26 17:32:04,222 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 0.0 (TID 2) (172.31.0.108, executor 1, partition 0, PROCESS_LOCAL, 4597 bytes) taskResourceAssignments Map()\n",
      "2022-04-26 17:32:04,223 INFO scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) on 172.31.0.108, executor 1: java.lang.ClassNotFoundException (org.apache.spark.sql.delta.files.DelayedCommitProtocol) [duplicate 1]\n",
      "2022-04-26 17:32:04,245 INFO scheduler.TaskSetManager: Starting task 1.1 in stage 0.0 (TID 3) (172.31.0.108, executor 1, partition 1, PROCESS_LOCAL, 4597 bytes) taskResourceAssignments Map()\n",
      "2022-04-26 17:32:04,245 INFO scheduler.TaskSetManager: Lost task 0.1 in stage 0.0 (TID 2) on 172.31.0.108, executor 1: java.lang.ClassNotFoundException (org.apache.spark.sql.delta.files.DelayedCommitProtocol) [duplicate 2]\n",
      "2022-04-26 17:32:04,328 INFO scheduler.TaskSetManager: Starting task 0.2 in stage 0.0 (TID 4) (172.31.0.108, executor 1, partition 0, PROCESS_LOCAL, 4597 bytes) taskResourceAssignments Map()\n",
      "2022-04-26 17:32:04,329 INFO scheduler.TaskSetManager: Lost task 1.1 in stage 0.0 (TID 3) on 172.31.0.108, executor 1: java.lang.ClassNotFoundException (org.apache.spark.sql.delta.files.DelayedCommitProtocol) [duplicate 3]\n",
      "2022-04-26 17:32:04,351 INFO scheduler.TaskSetManager: Starting task 1.2 in stage 0.0 (TID 5) (172.31.0.108, executor 1, partition 1, PROCESS_LOCAL, 4597 bytes) taskResourceAssignments Map()\n",
      "2022-04-26 17:32:04,351 INFO scheduler.TaskSetManager: Lost task 0.2 in stage 0.0 (TID 4) on 172.31.0.108, executor 1: java.lang.ClassNotFoundException (org.apache.spark.sql.delta.files.DelayedCommitProtocol) [duplicate 4]\n",
      "2022-04-26 17:32:04,425 INFO scheduler.TaskSetManager: Starting task 0.3 in stage 0.0 (TID 6) (172.31.0.108, executor 1, partition 0, PROCESS_LOCAL, 4597 bytes) taskResourceAssignments Map()\n",
      "2022-04-26 17:32:04,426 INFO scheduler.TaskSetManager: Lost task 1.2 in stage 0.0 (TID 5) on 172.31.0.108, executor 1: java.lang.ClassNotFoundException (org.apache.spark.sql.delta.files.DelayedCommitProtocol) [duplicate 5]\n",
      "2022-04-26 17:32:04,445 INFO scheduler.TaskSetManager: Starting task 1.3 in stage 0.0 (TID 7) (172.31.0.108, executor 1, partition 1, PROCESS_LOCAL, 4597 bytes) taskResourceAssignments Map()\n",
      "2022-04-26 17:32:04,445 INFO scheduler.TaskSetManager: Lost task 0.3 in stage 0.0 (TID 6) on 172.31.0.108, executor 1: java.lang.ClassNotFoundException (org.apache.spark.sql.delta.files.DelayedCommitProtocol) [duplicate 6]\n",
      "2022-04-26 17:32:04,446 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n",
      "2022-04-26 17:32:04,449 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0\n",
      "2022-04-26 17:32:04,449 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled\n",
      "2022-04-26 17:32:04,453 INFO scheduler.TaskSchedulerImpl: Stage 0 was cancelled\n",
      "2022-04-26 17:32:04,454 INFO scheduler.DAGScheduler: ResultStage 0 (save at NativeMethodAccessorImpl.java:0) failed in 1.634 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (172.31.0.108 executor 1): java.lang.ClassNotFoundException: org.apache.spark.sql.delta.files.DelayedCommitProtocol\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1995)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "2022-04-26 17:32:04,456 INFO scheduler.DAGScheduler: Job 0 failed: save at NativeMethodAccessorImpl.java:0, took 1.719491 s\n",
      "2022-04-26 17:32:04,458 ERROR datasources.FileFormatWriter: Aborting job 8964105d-9825-451b-96ec-a4d4f54ac24e.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (172.31.0.108 executor 1): java.lang.ClassNotFoundException: org.apache.spark.sql.delta.files.DelayedCommitProtocol\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1995)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:192)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:163)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:142)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:84)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:135)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:134)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:84)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:107)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:66)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:65)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:187)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:65)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:154)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:362)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.files.DelayedCommitProtocol\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1995)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "2022-04-26 17:32:04,534 WARN scheduler.TaskSetManager: Lost task 1.3 in stage 0.0 (TID 7) (172.31.0.108 executor 1): TaskKilled (Stage cancelled)\n",
      "2022-04-26 17:32:04,535 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/spark-f63da6df-8925-4139-bd88-f46115e64e17/spark_delta_lake.py\", line 14, in <module>\n",
      "    data.write.format(\"delta\").mode(\"overwrite\").save(\"v3io://projects/spark-delta/deltab\")\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1109, in save\n",
      "  File \"/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "  File \"/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o101.save.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:192)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:163)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:142)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:84)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:135)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:134)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:84)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:107)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:66)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:65)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:187)\n",
      "\tat org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:65)\n",
      "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:154)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:362)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (172.31.0.108 executor 1): java.lang.ClassNotFoundException: org.apache.spark.sql.delta.files.DelayedCommitProtocol\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1995)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\t... 50 more\n",
      "Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.files.DelayedCommitProtocol\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:68)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1995)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1862)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2169)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2102)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1667)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2464)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2358)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2196)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1679)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:493)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:451)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "\n",
      "2022-04-26 17:32:05,187 INFO server.AbstractConnector: Stopped Spark@27bab6a4{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2022-04-26 17:32:05,188 INFO ui.SparkUI: Stopped Spark web UI at http://read-delta-a0ee9dd5-03ec8f8066ed684c-driver-svc.default-tenant.svc:4040\n",
      "2022-04-26 17:32:05,190 INFO k8s.KubernetesClusterSchedulerBackend: Shutting down all executors\n",
      "2022-04-26 17:32:05,191 INFO k8s.KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down\n",
      "2022-04-26 17:32:05,196 WARN k8s.ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n",
      "2022-04-26 17:32:05,325 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2022-04-26 17:32:05,333 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2022-04-26 17:32:05,333 INFO storage.BlockManager: BlockManager stopped\n",
      "2022-04-26 17:32:05,339 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2022-04-26 17:32:05,343 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2022-04-26 17:32:05,347 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "2022-04-26 17:32:05,349 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "2022-04-26 17:32:05,349 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-e505979c-3004-422b-90d4-dcde5f7b8903/spark-1b38f426-b312-4ff5-b3b5-ca10c4b4c3ec\n",
      "2022-04-26 17:32:05,352 INFO util.ShutdownHookManager: Deleting directory /var/data/spark-e505979c-3004-422b-90d4-dcde5f7b8903/spark-1b38f426-b312-4ff5-b3b5-ca10c4b4c3ec/pyspark-be76a396-5fe6-45ba-ab6a-0a846d2b9f2a\n",
      "2022-04-26 17:32:05,355 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f63da6df-8925-4139-bd88-f46115e64e17\n",
      "final state: error\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".dictlist {\n",
       "  background-color: #4EC64B;\n",
       "  text-align: center;\n",
       "  margin: 4px;\n",
       "  border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;}\n",
       ".artifact {\n",
       "  cursor: pointer;\n",
       "  background-color: #4EC64B;\n",
       "  text-align: left;\n",
       "  margin: 4px; border-radius: 3px; padding: 0px 3px 1px 3px; display: inline-block;\n",
       "}\n",
       "div.block.hidden {\n",
       "  display: none;\n",
       "}\n",
       ".clickable {\n",
       "  cursor: pointer;\n",
       "}\n",
       ".ellipsis {\n",
       "  display: inline-block;\n",
       "  max-width: 60px;\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "}\n",
       ".master-wrapper {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: stretch;\n",
       "}\n",
       ".master-tbl {\n",
       "  flex: 3\n",
       "}\n",
       ".master-wrapper > div {\n",
       "  margin: 4px;\n",
       "  padding: 10px;\n",
       "}\n",
       "iframe.fileview {\n",
       "  border: 0 none;\n",
       "  height: 100%;\n",
       "  width: 100%;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       ".pane-header-title {\n",
       "  width: 80%;\n",
       "  font-weight: 500;\n",
       "}\n",
       ".pane-header {\n",
       "  line-height: 1;\n",
       "  background-color: #4EC64B;\n",
       "  padding: 3px;\n",
       "}\n",
       ".pane-header .close {\n",
       "  font-size: 20px;\n",
       "  font-weight: 700;\n",
       "  float: right;\n",
       "  margin-top: -5px;\n",
       "}\n",
       ".master-wrapper .right-pane {\n",
       "  border: 1px inset silver;\n",
       "  width: 40%;\n",
       "  min-height: 300px;\n",
       "  flex: 3\n",
       "  min-width: 500px;\n",
       "}\n",
       ".master-wrapper * {\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "</style><script>\n",
       "function copyToClipboard(fld) {\n",
       "    if (document.queryCommandSupported && document.queryCommandSupported('copy')) {\n",
       "        var textarea = document.createElement('textarea');\n",
       "        textarea.textContent = fld.innerHTML;\n",
       "        textarea.style.position = 'fixed';\n",
       "        document.body.appendChild(textarea);\n",
       "        textarea.select();\n",
       "\n",
       "        try {\n",
       "            return document.execCommand('copy'); // Security exception may be thrown by some browsers.\n",
       "        } catch (ex) {\n",
       "\n",
       "        } finally {\n",
       "            document.body.removeChild(textarea);\n",
       "        }\n",
       "    }\n",
       "}\n",
       "function expandPanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName');\n",
       "  console.log(el.title);\n",
       "\n",
       "  document.querySelector(panelName + \"-title\").innerHTML = el.title\n",
       "  iframe = document.querySelector(panelName + \"-body\");\n",
       "\n",
       "  const tblcss = `<style> body { font-family: Arial, Helvetica, sans-serif;}\n",
       "    #csv { margin-bottom: 15px; }\n",
       "    #csv table { border-collapse: collapse;}\n",
       "    #csv table td { padding: 4px 8px; border: 1px solid silver;} </style>`;\n",
       "\n",
       "  function csvToHtmlTable(str) {\n",
       "    return '<div id=\"csv\"><table><tr><td>' +  str.replace(/[\\n\\r]+$/g, '').replace(/[\\n\\r]+/g, '</td></tr><tr><td>')\n",
       "      .replace(/,/g, '</td><td>') + '</td></tr></table></div>';\n",
       "  }\n",
       "\n",
       "  function reqListener () {\n",
       "    if (el.title.endsWith(\".csv\")) {\n",
       "      iframe.setAttribute(\"srcdoc\", tblcss + csvToHtmlTable(this.responseText));\n",
       "    } else {\n",
       "      iframe.setAttribute(\"srcdoc\", this.responseText);\n",
       "    }\n",
       "    console.log(this.responseText);\n",
       "  }\n",
       "\n",
       "  const oReq = new XMLHttpRequest();\n",
       "  oReq.addEventListener(\"load\", reqListener);\n",
       "  oReq.open(\"GET\", el.title);\n",
       "  oReq.send();\n",
       "\n",
       "\n",
       "  //iframe.src = el.title;\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.remove(\"hidden\");\n",
       "  }\n",
       "}\n",
       "function closePanel(el) {\n",
       "  const panelName = \"#\" + el.getAttribute('paneName')\n",
       "  const resultPane = document.querySelector(panelName + \"-pane\");\n",
       "  if (!resultPane.classList.contains(\"hidden\")) {\n",
       "    resultPane.classList.add(\"hidden\");\n",
       "  }\n",
       "}\n",
       "\n",
       "</script>\n",
       "<div class=\"master-wrapper\">\n",
       "  <div class=\"block master-tbl\"><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>project</th>\n",
       "      <th>uid</th>\n",
       "      <th>iter</th>\n",
       "      <th>start</th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>labels</th>\n",
       "      <th>inputs</th>\n",
       "      <th>parameters</th>\n",
       "      <th>results</th>\n",
       "      <th>artifacts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>spark-delta</td>\n",
       "      <td><div title=\"7714d2fc2350485188dcff2f0d3af951\"><a href=\"https://dashboard.default-tenant.app.us-sales-322.iguazio-cd1.com/mlprojects/spark-delta/jobs/monitor/7714d2fc2350485188dcff2f0d3af951/overview\" target=\"_blank\" >...0d3af951</a></div></td>\n",
       "      <td>0</td>\n",
       "      <td>Apr 26 17:31:43</td>\n",
       "      <td><div style=\"color: red;\" title=\"\">error</div></td>\n",
       "      <td>read-delta</td>\n",
       "      <td><div class=\"dictlist\">v3io_user=brennan</div><div class=\"dictlist\">kind=spark</div><div class=\"dictlist\">owner=brennan</div><div class=\"dictlist\">mlrun/client_version=0.10.0</div><div class=\"dictlist\">mlrun/job=read-delta-a0ee9dd5</div><div class=\"dictlist\">host=read-delta-a0ee9dd5-driver</div></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></div>\n",
       "  <div id=\"result88673441-pane\" class=\"right-pane block hidden\">\n",
       "    <div class=\"pane-header\">\n",
       "      <span id=\"result88673441-title\" class=\"pane-header-title\">Title</span>\n",
       "      <span onclick=\"closePanel(this)\" paneName=\"result88673441\" class=\"close clickable\">&times;</span>\n",
       "    </div>\n",
       "    <iframe class=\"fileview\" id=\"result88673441-body\"></iframe>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b> > to track results use the .show() or .logs() methods  or <a href=\"https://dashboard.default-tenant.app.us-sales-322.iguazio-cd1.com/mlprojects/spark-delta/jobs/monitor/7714d2fc2350485188dcff2f0d3af951/overview\" target=\"_blank\">click here</a> to open in UI</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2022-04-26 13:32:47,760 [info] run executed, status=error\n",
      "runtime error: None\n"
     ]
    },
    {
     "ename": "RunError",
     "evalue": "None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRunError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c68123bf096a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/igz322/lib/python3.7/site-packages/mlrun/runtimes/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, runspec, handler, name, project, params, inputs, out_path, workdir, artifact_path, watch, schedule, hyperparams, hyper_param_options, verbose, scrape_metrics, local, local_code_path, auto_build)\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 )\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_run_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_remote\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_api_server\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/igz322/lib/python3.7/site-packages/mlrun/runtimes/base.py\u001b[0m in \u001b[0;36m_wrap_run_result\u001b[0;34m(self, result, runspec, schedule, err)\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_remote\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"runtime error: {run.status.error}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRunError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRunError\u001b[0m: None"
     ]
    }
   ],
   "source": [
    "fx.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e618b56bd61f1d0ab6764b1439ba5a7a6472cb1ca3835c51f53cbd65464aef8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('myprov': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
